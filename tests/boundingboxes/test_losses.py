import inspect
import math
import random

import pytest
import torch

from boundingboxes.losses import LogBarrierPenalty, InequalityL2Penalty, OutsideBoxEmptinessConstraintLoss, \
    BoxSizePriorLoss, BoxTightnessPriorLoss, CombinedLoss, Mode, Reduction

# Module marks
# ============

pytestmark = [
    # Disable warnings due to the use of torch.autograd.detect_anomaly
    pytest.mark.filterwarnings("ignore:Anomaly Detection has been enabled.*:UserWarning"),
    # Detect Monai's deprecation warnings
    pytest.mark.filterwarnings("ignore::DeprecationWarning:monai.utils.module"),
    # Remove masked tensor warnings
    pytest.mark.filterwarnings("ignore:The PyTorch API of MaskedTensors is in prototype stage:UserWarning"),
    pytest.mark.filterwarnings("ignore:It is not recommended to create a MaskedTensor with a tensor that requires_grad"
                               ":UserWarning"),
]


def skip_params(arg_name, skips):
    fixture = inspect.stack()[1][0].f_globals[arg_name]
    fixture_params = fixture._pytestfixturefunction.params  # noqa
    new_params = [param for param in fixture_params if param not in skips]
    return pytest.mark.parametrize(arg_name, new_params, indirect=True)


# Fixtures
# ========

@pytest.fixture(params=[
    'cpu',
    pytest.param('cuda', marks=pytest.mark.skipif(
        condition=not torch.cuda.is_available(),
        reason='No CUDA device available'
    ))
])
def device(request):
    return torch.device(request.param)


@pytest.fixture(params=['3D', '2D'])
def dimensions(request):
    return request.param.lower()


@pytest.fixture
def mode(dimensions):
    return Mode.MODE_3D if dimensions == '3d' else Mode.MODE_2D


# Logits and masks
# ----------------
@pytest.fixture
def shape(request, dimensions):
    # Match expected_boxes size and classes if requested, smaller otherwise
    if 'expected_boxes' in request.fixturenames:
        B, C, W, H = (1, 2, 6, 10)  # noqa
    else:
        B, C, W, H = (3, 4, 5, 5)  # noqa

    if dimensions == '3d':
        return B, C, W, H, H  # B x C x W x H x D
    if dimensions == '2d':
        return B, C, W, H  # B x C x W x H
    return NotImplemented


@pytest.fixture
def random_logits(device, shape):
    return torch.rand(*shape, device=device, requires_grad=True)


@pytest.fixture
def random_boxes(device, shape):
    """Generates a valid box mask for the losses, taking care of having only one box per mask, absent boxes and bg"""
    batch_size, num_classes, *im_shape = shape
    num_boxes = 3

    # To reflect real use cases, we'll have only one class with possibly 3 boxes, as well as an absent class, and the
    # rest with 1 box. This will result in many boxes to be absent and not specified in the sparse tensor.
    boxes = torch.zeros(batch_size, num_classes, num_boxes, *im_shape, dtype=torch.bool, device=device)
    for class_indices in [
        # No box in the background, we skip class 0
        # We'll keep one batch with no boxes, cover cases where a random crop yields a region with no segmentation
        # Class 1 has only one box
        *[(b, 1, 0) for b in range(batch_size - 1)],  # -1: Empty last batch
        # Class 2 in the first batch has 3 boxes
        *[(0, 2, n) for n in range(num_boxes)],
        # Claas 2 in the second batch has only 2 boxes (instead of 3, as in the first one)
        *[(1, 2, n) for n in range(num_boxes - 1)],
    ]:
        im_indices = torch.randint(0, 5, (len(im_shape), 2)).sort(dim=1).values
        im_slices = [slice(a, b + 1) for a, b in im_indices]
        boxes[*class_indices, *im_slices] = True
    return boxes.to_sparse(3)


@pytest.fixture
def empty_boxes(device, shape):
    # When no box is found in the crop, the box label is in fact empty as the N-dim (number of boxes) is 0
    batch_size, num_classes, *im_shape = shape
    return torch.zeros(batch_size, num_classes, 0, *im_shape, dtype=torch.bool, device=device).to_sparse(3)


@pytest.fixture
def expected_boxes_2d(device):
    # Expected boxes (2 boxes on a 6 by 10 image)
    # _ x x x _ _ _ _ _ _
    # _ x x x _ _ _ _ _ _
    # _ _ _ _ _ _ _ _ _ _
    # _ _ _ x x x x x _ _
    # _ _ _ x x x x x _ _
    # _ _ _ _ _ _ _ _ _ _
    boxes = torch.zeros(2, 6, 10, dtype=torch.bool, device=device)  # N x W x H
    boxes[0, 0:2, 1:4] = True
    boxes[1, 3:5, 3:8] = True

    # Add a background class with all 0s as generated by the transformations
    mask = torch.stack((torch.zeros_like(boxes, dtype=torch.bool), boxes))  # C x N x W x H

    # Adding batch and converting to sparse
    return torch.unsqueeze(mask, dim=0).to_sparse(3)


@pytest.fixture
def expected_boxes_3d(device):
    # Similar to 2D but with boxes overlapping on the third dimension with the pattern bellow
    # _ _ _ _ 1 1 1 1 _ _
    # _ _ 2 2 2 2 _ _ _ _
    boxes = torch.zeros(2, 6, 10, 10, dtype=torch.bool, device=device)  # N x W x H x D
    boxes[0, 0:2, 1:4, 4:8] = True
    boxes[1, 3:5, 3:8, 2:6] = True

    # Add a background class with all 0s as generated by the transformations
    mask = torch.stack((torch.zeros_like(boxes, dtype=torch.bool), boxes))  # C x N x W x H x D

    # Adding batch and converting to sparse
    return torch.unsqueeze(mask, dim=0).to_sparse(3)  # B x C x N x W x H x D


@pytest.fixture
def expected_boxes(dimensions, request):
    """Provides both 2D and 3D bounding boxes"""
    return request.getfixturevalue('expected_boxes_' + dimensions)


@pytest.fixture
def logits_gt(expected_boxes: torch.Tensor):
    return torch.any(expected_boxes.to_dense(), dim=2).float()


@pytest.fixture
def logits_zeros(shape, device):
    return torch.zeros(*shape, device=device)


# Loss factories
# --------------


# Do not use any penalty for this test
# Useful when comparing with 0 to ensure a known output
requires_a_zero_output = pytest.mark.parametrize('penalty', ['no-penalty', 'L2'], indirect=True)


@pytest.fixture(params=['log-barrier', 'L2', 'no-penalty'])
def penalty(request, device):
    if request.param == 'log-barrier':
        return LogBarrierPenalty().to(device)
    if request.param == 'L2':
        return InequalityL2Penalty().to(device)
    return lambda x: torch.maximum(x, torch.zeros_like(x))


@pytest.fixture(
    params=['none', 'mean', 'sum', 'original'],
    ids=lambda r: f'{r}-reduction' if r != 'none' else 'no-reduction'
)
def reduction(request):
    return Reduction(request.param)


@pytest.fixture
def box_tightness_loss(reduction, penalty, device, dimensions, mode):
    slices_width = 2
    return BoxTightnessPriorLoss(slices_width, penalty, reduction, mode).to(device)


@pytest.fixture
def box_size_loss(reduction, penalty, device):
    minimum = .3
    maximum = .75
    return BoxSizePriorLoss(minimum, maximum, penalty, reduction).to(device)


@pytest.fixture
def emptiness_constraint(reduction, penalty, device):
    return OutsideBoxEmptinessConstraintLoss(penalty, reduction).to(device)


# Tests
# =====
def test_backward_random():
    """Check that anomaly detection is enabled somehow if the name of the text contains the word 'backward'"""
    assert torch.is_anomaly_enabled() is True, "Anomaly detection should have been enabled for this test"


class TestPenalties:
    class TestLogBarrier:
        def test_log_barrier_penalty_t1(self, device):
            """
            Tests log barrier extension with t=1
            """
            # t=1, which implies a -1 threshold
            penalty = LogBarrierPenalty(t=1).to(device)
            torch.testing.assert_close(
                penalty(torch.tensor([-2., -1., 0., 1.], device=device)),
                torch.tensor([
                    # -1/t log(-z) => -log(-z)
                    -math.log(2),  # z=-2 => -log(2)
                    0.,
                    # tz - 1/t log(t/t**2) + 1/t => z + 1
                    1.,  # z=0 => 1
                    2.,  # z=1 => 2
                ], device=device)
            )

        def test_log_barrier_penalty_t2(self, device):
            """
            Tests log barrier extension with t=2
            """
            # t=2, which implies a -.25 threshold
            # tz + (1/t) * log(1/t**2) + 1/t  is the presented in the paper to illustrate continuity of the function.
            # It is actually equivalent to tz + (2/t) * log(t) + 1/t
            penalty = LogBarrierPenalty(t=2).to(device)
            torch.testing.assert_close(
                penalty(torch.tensor([-2., -1., -.3, -.2, 0., 1.], device=device)),
                torch.tensor([
                    # -1/t log(-z) => -log(-z)/2
                    -math.log(2) / 2,  # z=-2 => -log(2)
                    0.,
                    -math.log(.3) / 2,  # z=-.3 => -log(.3)  (just bellow 1/4)
                    # tz - 1/t log(t/t**2) + 1/t => 2*z + log(2) + .5
                    -2 * .2 + math.log(2) + .5,
                    math.log(2) + .5,  # z=0 => 1
                    2 + math.log(2) + .5,  # z=1 => 2
                ], device=device)
            )

        def test_log_barrier_nan(self, device, random_logits):
            penalty = LogBarrierPenalty().to(device)
            res = penalty(random_logits)
            assert bool(torch.isfinite(res).all()), 'Output must not contain NaN or Inf'

        def test_log_barrier_penalty_setting_t(self, device):
            penalty = LogBarrierPenalty(t=1).to(device)
            penalty.t = 2

        def test_log_barrier_penalty_stepping(self, device):
            penalty = LogBarrierPenalty(t=2).to(device)
            penalty.step()
            assert penalty.t == pytest.approx(2.2)

        @pytest.mark.parametrize('t', [1, 2, 5, 10, 25, 50])
        def test_log_barrier_penalty_grad(self, device, t):
            penalty = LogBarrierPenalty(t).to(device)
            input_ = torch.arange(-3, 3, 1, dtype=torch.float, requires_grad=True, device=device)
            res = penalty(input_)
            res.sum().backward()
            torch.testing.assert_close(
                input_.grad,
                torch.tensor([
                    1 / (3 * t),  # -3
                    1 / (2 * t),  # -2
                    1 / t,  # -1
                    t, t, t  # 0 to 2
                ], dtype=torch.float, device=device),
            )

        def test_log_barrier_penalty_rand_grad_check(self, device):
            penalty = LogBarrierPenalty(round(random.random() * 3 + 1, 2)).to(device)
            inputs = torch.rand(1, 1, dtype=torch.float64, requires_grad=True, device=device) * 4 - 3
            torch.autograd.gradcheck(penalty, inputs, atol=.1)

    class TestL2:
        def test_l2_penalty(self, device):
            penalty = InequalityL2Penalty().to(device)
            torch.testing.assert_close(
                penalty(torch.tensor([4, 5, 8, -6, -7, 1, 0], device=device)),
                torch.tensor([4 ** 2, 5 ** 2, 8 ** 2, 0, 0, 1, 0], device=device)
            )

    class TestAllPenalties:
        @skip_params('penalty', ['no-penalty'])
        def test_penalties_backward_forward(self, penalty, random_logits):
            penalized = penalty(random_logits)  # type: torch.Tensor

            assert penalized.requires_grad is True, 'The result of the penalty must require a gradient'
            assert penalized.is_leaf is False

            penalized.sum().backward()
            assert random_logits.grad is not None

        @skip_params('penalty', ['no-penalty'])
        def test_penalties_check_for_nan(self, penalty, random_logits):
            res = penalty(random_logits)
            assert bool(torch.isfinite(res).all()), 'Output must not contain NaN or Inf'


class TestLosses:
    class TestBoxTightness:
        @skip_params('penalty', ['log-barrier'])
        def test_box_tightness_gt(self, box_tightness_loss, expected_boxes, logits_gt, reduction):
            loss = box_tightness_loss(logits_gt, expected_boxes)
            torch.testing.assert_close(loss, torch.zeros_like(loss))

        def test_box_tightness_empty_bounding_boxes(self, box_tightness_loss, expected_boxes, logits_zeros, reduction):
            loss = box_tightness_loss(logits_zeros, expected_boxes)
            if reduction is not Reduction.NONE:
                assert loss.item() > 0
            else:
                assert torch.any(loss > 0).item() is True

        @pytest.mark.parametrize('reduction', ['mean', 'none'], indirect=True)
        @pytest.mark.parametrize('penalty', ['no-penalty'], indirect=True)
        def test_box_tightness_is_maximized_unmasked(self, box_tightness_loss, random_boxes, logits_zeros, reduction):
            """We want the mean Box Tightness to be one when the logits don't match the constraint at all"""
            loss = box_tightness_loss(logits_zeros, random_boxes)
            if reduction is Reduction.NONE:
                boxe_sizes = torch.sparse.sum(random_boxes.long(), dim=tuple(range(3, random_boxes.dim()))).to_dense()
                expected_ones = boxe_sizes.bool().float()
            else:
                expected_ones = torch.ones_like(loss)
            torch.testing.assert_close(loss, expected_ones)

        @skip_params('penalty', ['log-barrier'])
        def test_box_tightness_eye_3d(self, box_tightness_loss, reduction, device, mode):
            if mode is Mode.MODE_3D:
                # Expected box is a cubic 6 x 6 box at the center of the volume
                expected_boxes = torch.zeros(1, 2, 1, 10, 10, 10, dtype=torch.bool, device=device)
                expected_boxes[0, 1, 0, 2:8, 2:8, 2:8] = True

                # logits is a diagonal plane in the cube
                logits = torch.zeros(1, 2, 10, 10, 10, device=device)
                logits[0, 1, 2:8, 2:8, 2:8] = torch.eye(6).repeat(6, 1, 1)
            else:
                # Expected box is a cubic 6 x 6 box at the center of the area
                expected_boxes = torch.zeros(1, 2, 1, 10, 10, dtype=torch.bool, device=device)
                expected_boxes[0, 1, 0, 2:8, 2:8] = True

                # logits is a diagonal plane in the cube
                logits = torch.zeros(1, 2, 10, 10, device=device)
                logits[0, 1, 2:8, 2:8] = torch.eye(6)

            loss = box_tightness_loss(logits, expected_boxes.to_sparse(3))
            if reduction is not Reduction.NONE:
                assert loss.item() == 0
            else:
                assert torch.all(loss == 0).item() is True

    class TestBoxSize:
        def test_box_size_gt(self, box_size_loss, expected_boxes, logits_gt, reduction):
            loss = box_size_loss(logits_gt, expected_boxes)
            assert torch.isfinite(loss).all().item() is True
            if reduction is not Reduction.NONE:
                assert loss.item() > 0
            else:
                assert torch.any(loss > 0).item() is True

        def test_box_size_empty_bounding_boxes(self, box_size_loss, expected_boxes, logits_zeros, reduction):
            loss = box_size_loss(logits_zeros, expected_boxes)
            assert torch.isfinite(loss).all().item() is True
            if reduction is not Reduction.NONE:
                assert loss.item() > 0
            else:
                assert torch.any(loss > 0).item() is True

        @skip_params('penalty', ['log-barrier'])
        @pytest.mark.parametrize('dimensions', ['3D'], indirect=True)
        def test_box_size_correct_size_3d(self, box_size_loss, expected_boxes, logits_gt, reduction):
            logits_gt[..., 4:6] = 0  # predicts half of the bounding boxes
            loss = box_size_loss(logits_gt, expected_boxes)
            assert torch.isfinite(loss).all().item() is True
            if reduction is not Reduction.NONE:
                assert loss.item() == 0
            else:
                assert torch.all(loss == 0).item() is True

        @pytest.mark.parametrize('reduction', ['mean', 'sum'], indirect=True)
        @pytest.mark.parametrize('penalty', ['no-penalty'], indirect=True)
        @pytest.mark.xfail(reason='TODO cover N-boxes = 0, no info is info to cover classes not present in the dataset')
        def test_box_size_no_mask(self, box_size_loss, empty_boxes, shape, device):
            """If we don't have any mask, we should penalize all the pixels not being 0"""
            logits = torch.zeros(shape, device=device)
            loss = box_size_loss(logits, empty_boxes)
            assert bool(loss > 0) is True

    class TestEmptinessConstraint:
        @skip_params('penalty', ['log-barrier'])
        def test_emptiness_constraint_gt(self, emptiness_constraint, expected_boxes, logits_gt):
            """Test emptiness constraint with boxes given as logits 2D"""
            loss = emptiness_constraint(logits_gt, expected_boxes)
            assert loss.item() == 0

        def test_emptiness_constraint_gt_log_barrier_grad(self, emptiness_constraint, expected_boxes, logits_gt):
            """Test emptiness constraint with boxes given as logits 2D"""
            logits_gt.requires_grad = True
            loss = emptiness_constraint(logits_gt, expected_boxes)
            assert loss.item() >= 0

            loss.sum().backward()
            assert torch.isfinite(logits_gt.grad).all().item() is True
            assert (torch.all(logits_gt.grad[:, 0] == 0).item() is True,
                    "There must be no gradient for the background class")
            assert (torch.all(logits_gt.grad[logits_gt.bool()] == 0).item() is True,
                    "There must be no gradient in the boxes")
            assert (torch.all(logits_gt.grad[:, 1:][~logits_gt[:, 1:].bool()] > 0).item() is True,
                    "There must be gradient outside boxes")

        @skip_params('penalty', ['log-barrier'])
        def test_emptiness_constraint_only_fg(self, emptiness_constraint, expected_boxes, logits_zeros):
            """Test emptiness constraint with partial foreground in the bounding boxes 2D"""
            loss = emptiness_constraint(logits_zeros, expected_boxes)
            assert loss.item() == 0

        def test_emptiness_constraint_full_bg(self, emptiness_constraint, expected_boxes, shape, device):
            """Test emptiness constraint with background predicted as foreground"""
            logits = torch.ones(*shape, device=device)
            loss = emptiness_constraint(logits, expected_boxes)
            assert loss.item() > 0

    class TestAllLosses:
        """Global tests for all losses"""
        @pytest.fixture(params=['box_tightness_loss', 'box_size_loss', 'emptiness_constraint'],
                        ids=lambda name: name.replace('_', '-'))
        def any_loss(self, request, reduction, penalty, device, dimensions):  # Indirect fixtures used by losses
            return request.getfixturevalue(request.param)

        @pytest.fixture(params=['random_boxes', 'empty_boxes'])
        def any_boxes(self, request, shape, device):  # Indirect fixtures used by fixtures
            return request.getfixturevalue(request.param)

        def test_losses_backward_forward(self, any_loss, random_logits, any_boxes):
            loss = any_loss(random_logits, any_boxes)

            assert loss.requires_grad is True, 'The loss must require a gradient'
            assert loss.is_leaf is False

            loss.sum().backward()
            assert random_logits.grad is not None
            assert torch.isfinite(random_logits.grad).all().item() is True, 'Grad must not contain NaN or Inf'

        def test_losses_check_for_nan(self, any_loss, random_logits, any_boxes):
            res = any_loss(random_logits, any_boxes)
            assert bool(torch.isfinite(res).all()) is True, 'Output must not contain NaN or Inf'

        @pytest.mark.parametrize(('reduction', 'penalty'), [('mean', 'no-penalty')], indirect=True)
        def test_losses_output_clipping(self, any_loss, random_logits, random_boxes):
            """Ensures the casual output of the loss functions is always between 0 and 1, when there is no penalty"""
            res = any_loss(torch.nn.functional.softmax(random_logits, dim=1), random_boxes)
            assert 0 <= res.item() <= 1

    class TestCombinedLoss:
        @skip_params('reduction', ['none'])
        def test_combined_loss_nan(self, reduction, random_logits, random_boxes, device, dimensions, mode):
            loss_fn = CombinedLoss(reduction, mode=mode).to(device)
            res = loss_fn(random_logits, random_boxes)
            assert bool(torch.isfinite(res.weighted_loss).all()), 'Output must not contain NaN or Inf'
